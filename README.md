# Awesome Offensive Security AI

[简体中文](README_ZH.md)

A curated list of outstanding offensive security AI work, papers, organizations, libraries, and benchmarks.

## Benchmarks & Evaluation

### CTFTiny (Shao et al.)

A lightweight CTF benchmark with 50 challenges across various categories. (Submitted: Aug 5, 2025)

- [Towards Effective Offensive Security LLM Agents Paper](https://arxiv.org/abs/2508.05674)

### Cyber-Crowdsourced Elicitation

Evaluating AI cyber capabilities with crowdsourced elicitation. (Published: Jul 8, 2025)

- [Cyber-Crowdsourced Elicitation Blog Post](https://palisaderesearch.org/blog/cyber-crowdsourced-elicitation)

### CVE-Bench

CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Vulnerabilities. (Submitted: Mar 17, 2025)

- [CVE-Bench GitHub](https://github.com/uiuc-kang-lab/cve-bench)
- [CVE-Bench Paper](https://arxiv.org/abs/2503.17332)

### QHackBench

QHackBench: Benchmarking Large Language Models for Quantum Code Generation using PennyLane Hackathon Challenges. (Submitted: Jun 24, 2025)

- [QHackBench Paper](https://arxiv.org/abs/2506.20008)

### Bountybench

A framework to capture offensive & defensive cyber-capabilities in evolving real-world systems. (Submitted: May 21, 2025)

- [Bountybench Website](https://bountybench.github.io/)
- [Bountybench GitHub](https://github.com/bountybench/bountybench)
- [Bountybench Paper](https://arxiv.org/abs/2505.15216)

### nyuctf_agents

nyuctf_agents: Baseline LLM agents for NYU CTF benchmark. (Latest Release: Feb 6, 2025)

- [nyuctf_agents GitHub](https://github.com/NYU-LLM-CTF/nyuctf_agents)
- [NYU_CTF_BenchMark](https://github.com/NYU-LLM-CTF/NYU_CTF_Bench)

### InterCode-CTF

InterCode-CTF: A benchmark for evaluating LLMs in capture-the-flag challenges. (Submitted: Dec 3, 2024)

- [InterCode-CTF Blog Post](https://palisaderesearch.org/blog/intercode-ctf)
- [Paper: Hacking CTFs with Plain Agents](https://arxiv.org/abs/2412.02776)
- [intercode GitHub](https://github.com/palisaderesearch/intercode)

### Cybench

Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models. (Submitted: Aug 15, 2024)

- [Cybench Website](https://cybench.github.io/)
- [Cybench GitHub](https://github.com/andyzorigin/cybench)
- [Cybench Paper](https://arxiv.org/abs/2408.08926)
- [ICLR 2025 Oral Presentation](https://iclr.cc/virtual/2025/oral/31753)

### CYBERSECEVAL 3

CYBERSECEVAL 3: Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models. (Submitted: Aug, 2024)

- [CYBERSECEVAL 3 Paper](https://arxiv.org/abs/2408.10627)

### CyberMetric

CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge. (Submitted: Jul, 2024)

- [CyberMetric Paper](https://arxiv.org/abs/2407.08CyberMetric)
- [CyberMetric Dataset](https://huggingface.co/datasets/norbert-tihanyi/CyberMetric)

### SEvenLLM

SEvenLLM: A benchmark to elicit, and improve cybersecurity incident analysis and response abilities in LLMs for Security Events. (Submitted: May, 2024)

- [SEvenLLM Paper](https://arxiv.org/abs/2405.18354)
- [SEvenLLM GitHub](https://github.com/7evenllm/SEvenLLM)

### CyberSecEval_2

CYBERSECEVAL 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models. (Submitted: Apr, 2024)

- [CyberSecEval 2 Paper](https://arxiv.org/abs/2404.07920)
- [CyberSecEval 2 GitHub](https://github.com/meta-llama/cyberseceval)

### GDM Dangerous Capabilities

GDM Dangerous Capabilities: Capture the Flag. (Submitted: Mar, 2024)

- [GDM Dangerous Capabilities Paper](https://arxiv.org/abs/2403.13793)
- [GDM Dangerous Capabilities GitHub](https://github.com/google-deepmind/evals/tree/main/dangerous_capabilities)

### LLM_CTF

An Empirical Evaluation of LLMs for Solving Offensive Security Challenges. (Submitted: Feb 19, 2024)

- [LLM_CTF GitHub](https://github.com/NickNameInvalid/LLM_CTF)
- [LLM_CTF Paper](https://arxiv.org/abs/2402.11814)

### SecQA

SecQA: A Concise Question-Answering Dataset for Evaluating Large Language Models in Computer Security. (Submitted: Dec, 2023)

- [SecQA Paper](https://arxiv.org/abs/2312.07344)
- [SecQA Dataset](https://huggingface.co/datasets/secqa/secqa-v2)

## LLM Agents & Frameworks

### Towards Effective Offensive Security LLM Agents

Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark. (Submitted: Aug 5, 2025)

- [Towards Effective Offensive Security LLM Agents Paper](https://arxiv.org/abs/2508.05674)

#### CRAKEN

CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution. (Submitted: May 21, 2025)

- [CRAKEN GitHub](https://github.com/NYU-LLM-CTF/nyuctf_agents_craken)
- [CRAKEN Paper](https://arxiv.org/abs/2505.17107)

### RedTeamLLM

RedTeamLLM: an Agentic AI framework for offensive security. (Submitted: May 11, 2025)

- [RedTeamLLM Paper](https://arxiv.org/abs/2505.06913)
- [RedTeamLLM GitHub](https://github.com/lre-security-systems-team/redteamllm)

### HackSynth-GRPO

HackSynth-GRPO: A framework that enhances LLM agents for cryptographic CTF challenges using Guided Reinforcement Prompt Optimisation (GRPO). (Published: Apr 2025)

- [HackSynth-GRPO Paper](https://arxiv.org/html/2506.02048)
- [HackSynth-GRPO GitHub](https://github.com/aielte-research/HackSynth-GRPO)

### CAI

A lightweight, ergonomic framework for building bug bounty-ready Cybersecurity AIs (CAIs). (Paper Published: Apr 2025)

- [CAI Website](https://aliasrobotics.github.io/cai/)
- [CAI GitHub](https://github.com/aliasrobotics/cai)

#### D-CIPHER

D-CIPHER: Dynamic collaborative intelligent multi-agent system for offensive security. (Submitted: Feb 15, 2025)

- [D-CIPHER Paper](https://arxiv.org/abs/2502.10931)

### CTFTiny

CTFTINY: LITE BENCHMARKING OFFENSIVE CYBER SKILLS IN LARGE LANGUAGE MODELS. (Submitted: Feb 11, 2025)

- [CTFTiny GitHub](https://github.com/NYU-LLM-CTF/CTFTiny)

### PentestGPT

PentestGPT: A GPT-empowered penetration testing tool. (Published: Aug 12, 2024)

- [PentestGPT GitHub](https://github.com/GreyDGL/PentestGPT)
- [PentestGPT Paper](https://www.usenix.org/conference/usenixsecurity24/presentation/deng)

### HackSynth

HackSynth: LLM agent and evaluation framework for autonomous penetration testing. (Submitted: Dec 2, 2024)

- [HackSynth GitHub](https://github.com/aielte-research/HackSynth)
- [HackSynth Paper](https://arxiv.org/abs/2412.01778)

### EnIGMA

EnIGMA: A mode for solving offensive cybersecurity (capture the flag) challenges, achieving state-of-the-art results on multiple cybersecurity benchmarks. (Submitted: Sep 24, 2024)

- [EnIGMA Website](https://enigma-agent.com)
- [EnIGMA GitHub](https://github.com/SWE-agent)
- [EnIGMA Paper](https://arxiv.org/abs/2409.16165)
- [EnIGMA Benchmark](https://github.com/enigma-agent/benchmarks)

### XBOW(Business)

XBOW: A system that autonomously finds and exploits potential security vulnerabilities. (Initial commit: ~10 months ago)

- [XBOW Website](https://xbow.com/)
- [XBOW Benchmark](https://github.com/xbow-engineering/validation-benchmarks)
- [XBOW security advisory credits](https://github.com/advisories?query=credit%3Axbow-security)
- [AI Agents for OffSec with Zero False Positives (Black Hat 2025 Slides)](https://assets-global.website-files.com/658189b90f81ce5f1a7e6d63/66ac8e51655997008e75f733_XBOW%20-%20Black%20Hat%202025%20-%20AI%20Agents%20for%20Offsec%20with%20Zero%20False%20Positives.pdf)



## Competitions

### AI Cyber Challenge

AI Cyber Challenge: A competition to advance the state of the art in AI for cybersecurity.

- [AI Cyber Challenge Website](https://aicyberchallenge.com/)

## LLM Security Research & Tools

### Teaching LLMs how to XSS

Teaching LLMs how to XSS: An introduction to fine-tuning and reinforcement learning (using your own GPU)

- [Slides](https://docs.google.com/presentation/d/1feHRtOWdAKhZUQcfyzeDSgsx4Sn5QzqfgLFV1Tiskmo/edit)

### inspect_cyber

inspect_cyber: An Inspect extension developed by UKGovernmentBEIS, designed for agentic cyber evaluations. This project aims to facilitate the assessment of AI agents in cybersecurity contexts.

- [inspect_cyber Website](https://inspect.cyber.aisi.org.uk/)
- [inspect_cyber GitHub](https://github.com/UKGovernmentBEIS/inspect_cyber)

## How to Contribute

Contributions are always welcome! If you have a project, paper, or resource that fits this list, please feel free to open a pull request. Please ensure your submission adheres to the existing format and categories.
